# RagForge Services
# Usage: docker compose up -d
#
# Services:
# - Neo4j (knowledge graph database)
# - GLiNER (entity extraction - GPU accelerated)
# - TEI (text embeddings inference - GPU accelerated)
#
# To start without optional services:
#   docker compose up -d neo4j
#   docker compose up -d neo4j tei
#
# GPU Requirements:
# - NVIDIA GPU with CUDA support
# - nvidia-container-toolkit installed
# - Docker configured for GPU access

services:
  neo4j:
    image: neo4j:5.23-community
    container_name: ragforge-neo4j
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD:-ragforge}
      NEO4J_PLUGINS: '["apoc", "graph-data-science"]'
      NEO4J_server_memory_heap_initial__size: 512m
      NEO4J_server_memory_heap_max__size: 2G
      NEO4J_dbms_security_procedures_unrestricted: apoc.*,gds.*
    ports:
      - "${NEO4J_BOLT_PORT:-7687}:7687"
      - "${NEO4J_HTTP_PORT:-7474}:7474"
    volumes:
      - ragforge_neo4j_data:/data
      - ragforge_neo4j_logs:/logs
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:7474 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  gliner:
    build:
      context: ./gliner_service
      dockerfile: Dockerfile.gpu
    container_name: ragforge-gliner
    ports:
      - "${GLINER_PORT:-6971}:6971"
    environment:
      # GLINER_MODEL_NAME matches Python config.py (env_prefix = "GLINER_")
      # GLiNER2 models: base (205M, faster but more false positives) or large (340M, better accuracy)
      GLINER_MODEL_NAME: ${GLINER_MODEL_NAME:-fastino/gliner2-large-v1}
      GLINER_DEVICE: cuda
      GLINER_CONFIDENCE_THRESHOLD: ${GLINER_CONFIDENCE:-0.85}
    volumes:
      # Cache HuggingFace models to avoid re-downloading
      - ragforge_hf_cache:/root/.cache/huggingface
      # Mount Python code for hot-reload (no rebuild needed)
      - ./gliner_service:/app
      # Custom domain config (already in /app, kept for clarity)
      # - ./gliner_service/config:/app/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6971/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  tei:
    # Use turing tag for RTX 20xx GPUs (compute cap 7.5)
    # For RTX 30xx/40xx (Ampere/Ada), use tag "1.7" instead
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.7
    container_name: ragforge-tei
    ports:
      - "${TEI_PORT:-8081}:80"
    command:
      - --model-id
      - BAAI/bge-base-en-v1.5
    volumes:
      # Cache models
      - ragforge_tei_cache:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  ragforge_neo4j_data:
  ragforge_neo4j_logs:
  ragforge_hf_cache:
  ragforge_tei_cache:
